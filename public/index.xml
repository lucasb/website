<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Lucas Boscaini.com</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on Lucas Boscaini.com</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Feb 2026 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>About</title>
      <link>http://localhost:1313/about/</link>
      <pubDate>Mon, 02 Feb 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/about/</guid>
      <description>Who am I ? Lucas is a technologist and has been leading the research, discovery, and creation of new technologies, with experience in big techs and startups.&#xA;I am a builder and strategist focused on the intersection of technical innovation and practical execution. Throughout my career, I have navigated the high-scale environments of Big Tech and the high-velocity world of startups. This dual perspective has shaped my belief that great technology isn&amp;rsquo;t just about elegant code—it’s about solving the right problems at the right time.</description>
    </item>
    <item>
      <title>Talk Zero</title>
      <link>http://localhost:1313/talks/0/</link>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/talks/0/</guid>
      <description>my talk page Why these settings matter for ASIDE remove_unused_columns=False: By default, the Trainer drops any data not in the standard model&amp;rsquo;s forward signature. Since you added role_mask to your Gemma3AsideWrapper, you must keep this False so the mask reaches your custom loss function. paged_adamw_8bit: Because Gemma 3’s embedding layer is massive (256k tokens), even the optimizer states for the embeddings can be heavy. This optimizer keeps the model snappy on a single GPU.</description>
    </item>
    <item>
      <title>Untitled</title>
      <link>http://localhost:1313/posts/3/</link>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/3/</guid>
      <description>Intro Why these settings matter for ASIDE remove_unused_columns=False: By default, the Trainer drops any data not in the standard model&amp;rsquo;s forward signature. Since you added role_mask to your Gemma3AsideWrapper, you must keep this False so the mask reaches your custom loss function. paged_adamw_8bit: Because Gemma 3’s embedding layer is massive (256k tokens), even the optimizer states for the embeddings can be heavy. This optimizer keeps the model snappy on a single GPU.</description>
    </item>
    <item>
      <title>Untitled</title>
      <link>http://localhost:1313/posts/32/</link>
      <pubDate>Mon, 26 Jan 2026 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/32/</guid>
      <description>Intro Why these settings matter for ASIDE remove_unused_columns=False: By default, the Trainer drops any data not in the standard model&amp;rsquo;s forward signature. Since you added role_mask to your Gemma3AsideWrapper, you must keep this False so the mask reaches your custom loss function. paged_adamw_8bit: Because Gemma 3’s embedding layer is massive (256k tokens), even the optimizer states for the embeddings can be heavy. This optimizer keeps the model snappy on a single GPU.</description>
    </item>
  </channel>
</rss>
